{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "inputs=np.array([[73,67,43],\n",
    "               [91,88,64],\n",
    "               [87,134,58],\n",
    "               [102,43,37],\n",
    "               [69,96,70]],dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array([[56, 70], \n",
    "                    [81, 101], \n",
    "                    [119, 133], \n",
    "                    [22, 37], \n",
    "                    [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert numpy array to tensors\n",
    "\n",
    "inputs=torch.from_numpy(inputs)\n",
    "targets=torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 73.,  67.,  43.],\n",
       "        [ 91.,  88.,  64.],\n",
       "        [ 87., 134.,  58.],\n",
       "        [102.,  43.,  37.],\n",
       "        [ 69.,  96.,  70.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7573, -1.4154], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#weights and biases \n",
    "\n",
    "w=torch.randn(2,3,requires_grad=True)\n",
    "b=torch.randn(2,requires_grad=True)\n",
    "w\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9622, -1.2770, -1.0146],\n",
       "        [ 0.8352,  0.4804,  1.2710]], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7573, -1.4154], requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x @ w.t() + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-200.1888,  146.3913],\n",
      "        [-265.6328,  198.2036],\n",
      "        [-314.4403,  209.3359],\n",
      "        [-191.3572,  151.4550],\n",
      "        [-260.7673,  191.2995]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#predictions\n",
    "\n",
    "preds=model(inputs)\n",
    "print(preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t1,t2):\n",
    "    diff=t1-t2\n",
    "    return torch.mean((diff**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=mse(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(59094.3945, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute gradients\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w-=w.grad*1e-5\n",
    "    b-=b.grad*1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    preds = model(inputs)\n",
    "    loss = mse(preds, targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= w.grad * 1e-5\n",
    "        b -= b.grad * 1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(216.1115, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate loss\n",
    "preds = model(inputs)\n",
    "loss = mse(preds, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression Using pytorch built in\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], \n",
    "                   [102, 43, 37], [69, 96, 70], [73, 67, 43], \n",
    "                   [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70], [73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70]], \n",
    "                  dtype='float32')\n",
    "\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], \n",
    "                    [22, 37], [103, 119], [56, 70], \n",
    "                    [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119], [56, 70], [81, 101], \n",
    "                    [119, 133], [22, 37], [103, 119]], \n",
    "                   dtype='float32')\n",
    "\n",
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Dataset\n",
    "train_ds=TensorDataset(inputs,targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]), tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5\n",
    "train_dl=DataLoader(train_ds,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 69.,  96.,  70.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[103., 119.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.],\n",
      "        [ 22.,  37.]])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 91.,  88.,  64.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[ 81., 101.],\n",
      "        [103., 119.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.],\n",
      "        [ 22.,  37.]])\n",
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [102.,  43.,  37.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 56.,  70.],\n",
      "        [119., 133.],\n",
      "        [103., 119.],\n",
      "        [ 22.,  37.]])\n",
      "tensor([[ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 91.,  88.,  64.]])\n",
      "tensor([[119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.],\n",
      "        [119., 133.],\n",
      "        [ 81., 101.]])\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(xb)\n",
    "    print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1781, -0.2776,  0.1820],\n",
      "        [ 0.1126,  0.2353,  0.4949]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4985, -0.3365], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Define model\n",
    "model=nn.Linear(3,2)\n",
    "print(model.weight)\n",
    "print(model.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.1781, -0.2776,  0.1820],\n",
       "         [ 0.1126,  0.2353,  0.4949]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([ 0.4985, -0.3365], requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  2.7262,  44.9276],\n",
       "        [  3.9245,  62.2886],\n",
       "        [-10.6493,  69.6928],\n",
       "        [ 13.4607,  39.5748],\n",
       "        [ -1.1219,  64.6644],\n",
       "        [  2.7262,  44.9276],\n",
       "        [  3.9245,  62.2886],\n",
       "        [-10.6493,  69.6928],\n",
       "        [ 13.4607,  39.5748],\n",
       "        [ -1.1219,  64.6644],\n",
       "        [  2.7262,  44.9276],\n",
       "        [  3.9245,  62.2886],\n",
       "        [-10.6493,  69.6928],\n",
       "        [ 13.4607,  39.5748],\n",
       "        [ -1.1219,  64.6644]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing nn.Functional\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn=F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=loss_fn(model(inputs),targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Optimizer\n",
    "opt=torch.optim.SGD(model.parameters(),lr=1e-5) #Note that model.parameters() is passed as an argument to optim.SGD, so that the optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which the parameters are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epochs,model,loss_fn,opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            \n",
    "            pred=model(xb)\n",
    "            \n",
    "            loss=loss_fn(pred,yb)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            opt.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            opt.zero_grad()\n",
    "            \n",
    "             # Print the progress\n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10000], Loss: 0.7990\n",
      "Epoch [20/10000], Loss: 0.9876\n",
      "Epoch [30/10000], Loss: 0.4639\n",
      "Epoch [40/10000], Loss: 0.7274\n",
      "Epoch [50/10000], Loss: 0.7176\n",
      "Epoch [60/10000], Loss: 0.7270\n",
      "Epoch [70/10000], Loss: 0.8529\n",
      "Epoch [80/10000], Loss: 0.4334\n",
      "Epoch [90/10000], Loss: 1.0042\n",
      "Epoch [100/10000], Loss: 0.9836\n",
      "Epoch [110/10000], Loss: 0.8082\n",
      "Epoch [120/10000], Loss: 0.8062\n",
      "Epoch [130/10000], Loss: 0.5981\n",
      "Epoch [140/10000], Loss: 0.7731\n",
      "Epoch [150/10000], Loss: 0.6554\n",
      "Epoch [160/10000], Loss: 0.6540\n",
      "Epoch [170/10000], Loss: 0.6085\n",
      "Epoch [180/10000], Loss: 0.4206\n",
      "Epoch [190/10000], Loss: 0.7621\n",
      "Epoch [200/10000], Loss: 0.4654\n",
      "Epoch [210/10000], Loss: 0.6062\n",
      "Epoch [220/10000], Loss: 0.6887\n",
      "Epoch [230/10000], Loss: 0.3465\n",
      "Epoch [240/10000], Loss: 0.7346\n",
      "Epoch [250/10000], Loss: 0.4812\n",
      "Epoch [260/10000], Loss: 0.4026\n",
      "Epoch [270/10000], Loss: 0.6216\n",
      "Epoch [280/10000], Loss: 0.6056\n",
      "Epoch [290/10000], Loss: 0.5037\n",
      "Epoch [300/10000], Loss: 0.7464\n",
      "Epoch [310/10000], Loss: 0.3726\n",
      "Epoch [320/10000], Loss: 0.3643\n",
      "Epoch [330/10000], Loss: 0.5656\n",
      "Epoch [340/10000], Loss: 0.5395\n",
      "Epoch [350/10000], Loss: 0.5060\n",
      "Epoch [360/10000], Loss: 0.6110\n",
      "Epoch [370/10000], Loss: 0.6117\n",
      "Epoch [380/10000], Loss: 0.4213\n",
      "Epoch [390/10000], Loss: 0.5906\n",
      "Epoch [400/10000], Loss: 0.5012\n",
      "Epoch [410/10000], Loss: 0.4972\n",
      "Epoch [420/10000], Loss: 0.4550\n",
      "Epoch [430/10000], Loss: 0.3799\n",
      "Epoch [440/10000], Loss: 0.6752\n",
      "Epoch [450/10000], Loss: 0.4557\n",
      "Epoch [460/10000], Loss: 0.5911\n",
      "Epoch [470/10000], Loss: 0.6575\n",
      "Epoch [480/10000], Loss: 0.4702\n",
      "Epoch [490/10000], Loss: 0.5262\n",
      "Epoch [500/10000], Loss: 0.5036\n",
      "Epoch [510/10000], Loss: 0.4672\n",
      "Epoch [520/10000], Loss: 0.5252\n",
      "Epoch [530/10000], Loss: 0.7444\n",
      "Epoch [540/10000], Loss: 0.4534\n",
      "Epoch [550/10000], Loss: 0.6297\n",
      "Epoch [560/10000], Loss: 0.4775\n",
      "Epoch [570/10000], Loss: 0.7395\n",
      "Epoch [580/10000], Loss: 0.2885\n",
      "Epoch [590/10000], Loss: 0.5645\n",
      "Epoch [600/10000], Loss: 0.3650\n",
      "Epoch [610/10000], Loss: 0.6669\n",
      "Epoch [620/10000], Loss: 0.3863\n",
      "Epoch [630/10000], Loss: 0.5164\n",
      "Epoch [640/10000], Loss: 0.4625\n",
      "Epoch [650/10000], Loss: 0.6659\n",
      "Epoch [660/10000], Loss: 0.5600\n",
      "Epoch [670/10000], Loss: 0.5987\n",
      "Epoch [680/10000], Loss: 0.6659\n",
      "Epoch [690/10000], Loss: 0.6508\n",
      "Epoch [700/10000], Loss: 0.5986\n",
      "Epoch [710/10000], Loss: 0.7900\n",
      "Epoch [720/10000], Loss: 0.5223\n",
      "Epoch [730/10000], Loss: 0.4820\n",
      "Epoch [740/10000], Loss: 0.6435\n",
      "Epoch [750/10000], Loss: 0.5189\n",
      "Epoch [760/10000], Loss: 0.5186\n",
      "Epoch [770/10000], Loss: 0.5598\n",
      "Epoch [780/10000], Loss: 0.5183\n",
      "Epoch [790/10000], Loss: 0.4515\n",
      "Epoch [800/10000], Loss: 0.4926\n",
      "Epoch [810/10000], Loss: 0.6898\n",
      "Epoch [820/10000], Loss: 0.3675\n",
      "Epoch [830/10000], Loss: 0.4135\n",
      "Epoch [840/10000], Loss: 0.3604\n",
      "Epoch [850/10000], Loss: 0.6607\n",
      "Epoch [860/10000], Loss: 0.5448\n",
      "Epoch [870/10000], Loss: 0.7175\n",
      "Epoch [880/10000], Loss: 0.5131\n",
      "Epoch [890/10000], Loss: 0.5174\n",
      "Epoch [900/10000], Loss: 0.3999\n",
      "Epoch [910/10000], Loss: 0.6626\n",
      "Epoch [920/10000], Loss: 0.4601\n",
      "Epoch [930/10000], Loss: 0.5856\n",
      "Epoch [940/10000], Loss: 0.4687\n",
      "Epoch [950/10000], Loss: 0.4652\n",
      "Epoch [960/10000], Loss: 0.6288\n",
      "Epoch [970/10000], Loss: 0.4842\n",
      "Epoch [980/10000], Loss: 0.5984\n",
      "Epoch [990/10000], Loss: 0.6787\n",
      "Epoch [1000/10000], Loss: 0.4592\n",
      "Epoch [1010/10000], Loss: 0.4958\n",
      "Epoch [1020/10000], Loss: 0.6917\n",
      "Epoch [1030/10000], Loss: 0.4483\n",
      "Epoch [1040/10000], Loss: 0.6571\n",
      "Epoch [1050/10000], Loss: 0.5934\n",
      "Epoch [1060/10000], Loss: 0.5794\n",
      "Epoch [1070/10000], Loss: 0.5168\n",
      "Epoch [1080/10000], Loss: 0.6321\n",
      "Epoch [1090/10000], Loss: 0.6229\n",
      "Epoch [1100/10000], Loss: 0.4721\n",
      "Epoch [1110/10000], Loss: 0.6210\n",
      "Epoch [1120/10000], Loss: 0.4414\n",
      "Epoch [1130/10000], Loss: 0.5536\n",
      "Epoch [1140/10000], Loss: 0.5165\n",
      "Epoch [1150/10000], Loss: 0.4728\n",
      "Epoch [1160/10000], Loss: 0.4978\n",
      "Epoch [1170/10000], Loss: 0.6599\n",
      "Epoch [1180/10000], Loss: 0.3610\n",
      "Epoch [1190/10000], Loss: 0.5635\n",
      "Epoch [1200/10000], Loss: 0.7002\n",
      "Epoch [1210/10000], Loss: 0.3762\n",
      "Epoch [1220/10000], Loss: 0.4190\n",
      "Epoch [1230/10000], Loss: 0.4560\n",
      "Epoch [1240/10000], Loss: 0.4323\n",
      "Epoch [1250/10000], Loss: 0.5514\n",
      "Epoch [1260/10000], Loss: 0.4218\n",
      "Epoch [1270/10000], Loss: 0.7342\n",
      "Epoch [1280/10000], Loss: 0.4666\n",
      "Epoch [1290/10000], Loss: 0.5164\n",
      "Epoch [1300/10000], Loss: 0.4139\n",
      "Epoch [1310/10000], Loss: 0.5706\n",
      "Epoch [1320/10000], Loss: 0.6783\n",
      "Epoch [1330/10000], Loss: 0.4121\n",
      "Epoch [1340/10000], Loss: 0.4956\n",
      "Epoch [1350/10000], Loss: 0.5165\n",
      "Epoch [1360/10000], Loss: 0.4677\n",
      "Epoch [1370/10000], Loss: 0.5165\n",
      "Epoch [1380/10000], Loss: 0.6049\n",
      "Epoch [1390/10000], Loss: 0.5744\n",
      "Epoch [1400/10000], Loss: 0.5500\n",
      "Epoch [1410/10000], Loss: 0.5945\n",
      "Epoch [1420/10000], Loss: 0.4014\n",
      "Epoch [1430/10000], Loss: 0.5844\n",
      "Epoch [1440/10000], Loss: 0.4885\n",
      "Epoch [1450/10000], Loss: 0.4909\n",
      "Epoch [1460/10000], Loss: 0.5296\n",
      "Epoch [1470/10000], Loss: 0.8025\n",
      "Epoch [1480/10000], Loss: 0.4829\n",
      "Epoch [1490/10000], Loss: 0.6118\n",
      "Epoch [1500/10000], Loss: 0.5172\n",
      "Epoch [1510/10000], Loss: 0.6258\n",
      "Epoch [1520/10000], Loss: 0.5514\n",
      "Epoch [1530/10000], Loss: 0.5401\n",
      "Epoch [1540/10000], Loss: 0.4298\n",
      "Epoch [1550/10000], Loss: 0.4643\n",
      "Epoch [1560/10000], Loss: 0.6031\n",
      "Epoch [1570/10000], Loss: 0.5741\n",
      "Epoch [1580/10000], Loss: 0.6618\n",
      "Epoch [1590/10000], Loss: 0.6131\n",
      "Epoch [1600/10000], Loss: 0.5154\n",
      "Epoch [1610/10000], Loss: 0.6871\n",
      "Epoch [1620/10000], Loss: 0.6778\n",
      "Epoch [1630/10000], Loss: 0.6669\n",
      "Epoch [1640/10000], Loss: 0.3702\n",
      "Epoch [1650/10000], Loss: 0.6178\n",
      "Epoch [1660/10000], Loss: 0.5163\n",
      "Epoch [1670/10000], Loss: 0.5410\n",
      "Epoch [1680/10000], Loss: 0.6127\n",
      "Epoch [1690/10000], Loss: 0.6392\n",
      "Epoch [1700/10000], Loss: 0.4962\n",
      "Epoch [1710/10000], Loss: 0.6623\n",
      "Epoch [1720/10000], Loss: 0.5167\n",
      "Epoch [1730/10000], Loss: 0.6787\n",
      "Epoch [1740/10000], Loss: 0.3844\n",
      "Epoch [1750/10000], Loss: 0.6811\n",
      "Epoch [1760/10000], Loss: 0.4840\n",
      "Epoch [1770/10000], Loss: 0.6073\n",
      "Epoch [1780/10000], Loss: 0.6068\n",
      "Epoch [1790/10000], Loss: 0.5684\n",
      "Epoch [1800/10000], Loss: 0.5406\n",
      "Epoch [1810/10000], Loss: 0.5163\n",
      "Epoch [1820/10000], Loss: 0.5163\n",
      "Epoch [1830/10000], Loss: 0.6779\n",
      "Epoch [1840/10000], Loss: 0.4313\n",
      "Epoch [1850/10000], Loss: 0.5849\n",
      "Epoch [1860/10000], Loss: 0.5163\n",
      "Epoch [1870/10000], Loss: 0.4873\n",
      "Epoch [1880/10000], Loss: 0.4679\n",
      "Epoch [1890/10000], Loss: 0.4901\n",
      "Epoch [1900/10000], Loss: 0.3488\n",
      "Epoch [1910/10000], Loss: 0.6604\n",
      "Epoch [1920/10000], Loss: 0.7171\n",
      "Epoch [1930/10000], Loss: 0.6279\n",
      "Epoch [1940/10000], Loss: 0.6635\n",
      "Epoch [1950/10000], Loss: 0.4886\n",
      "Epoch [1960/10000], Loss: 0.6014\n",
      "Epoch [1970/10000], Loss: 0.7893\n",
      "Epoch [1980/10000], Loss: 0.6074\n",
      "Epoch [1990/10000], Loss: 0.5163\n",
      "Epoch [2000/10000], Loss: 0.4633\n",
      "Epoch [2010/10000], Loss: 0.6010\n",
      "Epoch [2020/10000], Loss: 0.5886\n",
      "Epoch [2030/10000], Loss: 0.5934\n",
      "Epoch [2040/10000], Loss: 0.5163\n",
      "Epoch [2050/10000], Loss: 0.5936\n",
      "Epoch [2060/10000], Loss: 0.7102\n",
      "Epoch [2070/10000], Loss: 0.4014\n",
      "Epoch [2080/10000], Loss: 0.5163\n",
      "Epoch [2090/10000], Loss: 0.5277\n",
      "Epoch [2100/10000], Loss: 0.5941\n",
      "Epoch [2110/10000], Loss: 0.4978\n",
      "Epoch [2120/10000], Loss: 0.6104\n",
      "Epoch [2130/10000], Loss: 0.5102\n",
      "Epoch [2140/10000], Loss: 0.3766\n",
      "Epoch [2150/10000], Loss: 0.6837\n",
      "Epoch [2160/10000], Loss: 0.4847\n",
      "Epoch [2170/10000], Loss: 0.6748\n",
      "Epoch [2180/10000], Loss: 0.3715\n",
      "Epoch [2190/10000], Loss: 0.5376\n",
      "Epoch [2200/10000], Loss: 0.5943\n",
      "Epoch [2210/10000], Loss: 0.6755\n",
      "Epoch [2220/10000], Loss: 0.4633\n",
      "Epoch [2230/10000], Loss: 0.5164\n",
      "Epoch [2240/10000], Loss: 0.4985\n",
      "Epoch [2250/10000], Loss: 0.6317\n",
      "Epoch [2260/10000], Loss: 0.5379\n",
      "Epoch [2270/10000], Loss: 0.7336\n",
      "Epoch [2280/10000], Loss: 0.4741\n",
      "Epoch [2290/10000], Loss: 0.4293\n",
      "Epoch [2300/10000], Loss: 0.5163\n",
      "Epoch [2310/10000], Loss: 0.6019\n",
      "Epoch [2320/10000], Loss: 0.4593\n",
      "Epoch [2330/10000], Loss: 0.4894\n",
      "Epoch [2340/10000], Loss: 0.5089\n",
      "Epoch [2350/10000], Loss: 0.4118\n",
      "Epoch [2360/10000], Loss: 0.5976\n",
      "Epoch [2370/10000], Loss: 0.6735\n",
      "Epoch [2380/10000], Loss: 0.5468\n",
      "Epoch [2390/10000], Loss: 0.7864\n",
      "Epoch [2400/10000], Loss: 0.6005\n",
      "Epoch [2410/10000], Loss: 0.6859\n",
      "Epoch [2420/10000], Loss: 0.8111\n",
      "Epoch [2430/10000], Loss: 0.5044\n",
      "Epoch [2440/10000], Loss: 0.7107\n",
      "Epoch [2450/10000], Loss: 0.8210\n",
      "Epoch [2460/10000], Loss: 0.6257\n",
      "Epoch [2470/10000], Loss: 0.4775\n",
      "Epoch [2480/10000], Loss: 0.4177\n",
      "Epoch [2490/10000], Loss: 0.6078\n",
      "Epoch [2500/10000], Loss: 0.4909\n",
      "Epoch [2510/10000], Loss: 0.6806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2520/10000], Loss: 0.5945\n",
      "Epoch [2530/10000], Loss: 0.6224\n",
      "Epoch [2540/10000], Loss: 0.5760\n",
      "Epoch [2550/10000], Loss: 0.3651\n",
      "Epoch [2560/10000], Loss: 0.4555\n",
      "Epoch [2570/10000], Loss: 0.6049\n",
      "Epoch [2580/10000], Loss: 0.3736\n",
      "Epoch [2590/10000], Loss: 0.5958\n",
      "Epoch [2600/10000], Loss: 0.6244\n",
      "Epoch [2610/10000], Loss: 0.7951\n",
      "Epoch [2620/10000], Loss: 0.4774\n",
      "Epoch [2630/10000], Loss: 0.7508\n",
      "Epoch [2640/10000], Loss: 0.5388\n",
      "Epoch [2650/10000], Loss: 0.5907\n",
      "Epoch [2660/10000], Loss: 0.6666\n",
      "Epoch [2670/10000], Loss: 0.5162\n",
      "Epoch [2680/10000], Loss: 0.4650\n",
      "Epoch [2690/10000], Loss: 0.4039\n",
      "Epoch [2700/10000], Loss: 0.3669\n",
      "Epoch [2710/10000], Loss: 0.5872\n",
      "Epoch [2720/10000], Loss: 0.5027\n",
      "Epoch [2730/10000], Loss: 0.4928\n",
      "Epoch [2740/10000], Loss: 0.6696\n",
      "Epoch [2750/10000], Loss: 0.5950\n",
      "Epoch [2760/10000], Loss: 0.6355\n",
      "Epoch [2770/10000], Loss: 0.4330\n",
      "Epoch [2780/10000], Loss: 0.5163\n",
      "Epoch [2790/10000], Loss: 0.7913\n",
      "Epoch [2800/10000], Loss: 0.4909\n",
      "Epoch [2810/10000], Loss: 0.6891\n",
      "Epoch [2820/10000], Loss: 0.4402\n",
      "Epoch [2830/10000], Loss: 0.5165\n",
      "Epoch [2840/10000], Loss: 0.5166\n",
      "Epoch [2850/10000], Loss: 0.4980\n",
      "Epoch [2860/10000], Loss: 0.5164\n",
      "Epoch [2870/10000], Loss: 0.5009\n",
      "Epoch [2880/10000], Loss: 0.6816\n",
      "Epoch [2890/10000], Loss: 0.7326\n",
      "Epoch [2900/10000], Loss: 0.5497\n",
      "Epoch [2910/10000], Loss: 0.4815\n",
      "Epoch [2920/10000], Loss: 0.4531\n",
      "Epoch [2930/10000], Loss: 0.6739\n",
      "Epoch [2940/10000], Loss: 0.5436\n",
      "Epoch [2950/10000], Loss: 0.6705\n",
      "Epoch [2960/10000], Loss: 0.5099\n",
      "Epoch [2970/10000], Loss: 0.5769\n",
      "Epoch [2980/10000], Loss: 0.4016\n",
      "Epoch [2990/10000], Loss: 0.5920\n",
      "Epoch [3000/10000], Loss: 0.5162\n",
      "Epoch [3010/10000], Loss: 0.3812\n",
      "Epoch [3020/10000], Loss: 0.5793\n",
      "Epoch [3030/10000], Loss: 0.4881\n",
      "Epoch [3040/10000], Loss: 0.6213\n",
      "Epoch [3050/10000], Loss: 0.4167\n",
      "Epoch [3060/10000], Loss: 0.5499\n",
      "Epoch [3070/10000], Loss: 0.5235\n",
      "Epoch [3080/10000], Loss: 0.6031\n",
      "Epoch [3090/10000], Loss: 0.5163\n",
      "Epoch [3100/10000], Loss: 0.6149\n",
      "Epoch [3110/10000], Loss: 0.7903\n",
      "Epoch [3120/10000], Loss: 0.5165\n",
      "Epoch [3130/10000], Loss: 0.3695\n",
      "Epoch [3140/10000], Loss: 0.5357\n",
      "Epoch [3150/10000], Loss: 0.3749\n",
      "Epoch [3160/10000], Loss: 0.4663\n",
      "Epoch [3170/10000], Loss: 0.5163\n",
      "Epoch [3180/10000], Loss: 0.4980\n",
      "Epoch [3190/10000], Loss: 0.5104\n",
      "Epoch [3200/10000], Loss: 0.3782\n",
      "Epoch [3210/10000], Loss: 0.6777\n",
      "Epoch [3220/10000], Loss: 0.2901\n",
      "Epoch [3230/10000], Loss: 0.4943\n",
      "Epoch [3240/10000], Loss: 0.4403\n",
      "Epoch [3250/10000], Loss: 0.6031\n",
      "Epoch [3260/10000], Loss: 0.3853\n",
      "Epoch [3270/10000], Loss: 0.5163\n",
      "Epoch [3280/10000], Loss: 0.6168\n",
      "Epoch [3290/10000], Loss: 0.4330\n",
      "Epoch [3300/10000], Loss: 0.4288\n",
      "Epoch [3310/10000], Loss: 0.6938\n",
      "Epoch [3320/10000], Loss: 0.4588\n",
      "Epoch [3330/10000], Loss: 0.3393\n",
      "Epoch [3340/10000], Loss: 0.5108\n",
      "Epoch [3350/10000], Loss: 0.6695\n",
      "Epoch [3360/10000], Loss: 0.5504\n",
      "Epoch [3370/10000], Loss: 0.4127\n",
      "Epoch [3380/10000], Loss: 0.6008\n",
      "Epoch [3390/10000], Loss: 0.8021\n",
      "Epoch [3400/10000], Loss: 0.4067\n",
      "Epoch [3410/10000], Loss: 0.6064\n",
      "Epoch [3420/10000], Loss: 0.6621\n",
      "Epoch [3430/10000], Loss: 0.3708\n",
      "Epoch [3440/10000], Loss: 0.4137\n",
      "Epoch [3450/10000], Loss: 0.6680\n",
      "Epoch [3460/10000], Loss: 0.4422\n",
      "Epoch [3470/10000], Loss: 0.4088\n",
      "Epoch [3480/10000], Loss: 0.5760\n",
      "Epoch [3490/10000], Loss: 0.4516\n",
      "Epoch [3500/10000], Loss: 0.7450\n",
      "Epoch [3510/10000], Loss: 0.5453\n",
      "Epoch [3520/10000], Loss: 0.6175\n",
      "Epoch [3530/10000], Loss: 0.5370\n",
      "Epoch [3540/10000], Loss: 0.4911\n",
      "Epoch [3550/10000], Loss: 0.5777\n",
      "Epoch [3560/10000], Loss: 0.6873\n",
      "Epoch [3570/10000], Loss: 0.6787\n",
      "Epoch [3580/10000], Loss: 0.8135\n",
      "Epoch [3590/10000], Loss: 0.4596\n",
      "Epoch [3600/10000], Loss: 0.6109\n",
      "Epoch [3610/10000], Loss: 0.4762\n",
      "Epoch [3620/10000], Loss: 0.5281\n",
      "Epoch [3630/10000], Loss: 0.4752\n",
      "Epoch [3640/10000], Loss: 0.5513\n",
      "Epoch [3650/10000], Loss: 0.4976\n",
      "Epoch [3660/10000], Loss: 0.6492\n",
      "Epoch [3670/10000], Loss: 0.4915\n",
      "Epoch [3680/10000], Loss: 0.5744\n",
      "Epoch [3690/10000], Loss: 0.7881\n",
      "Epoch [3700/10000], Loss: 0.5084\n",
      "Epoch [3710/10000], Loss: 0.5163\n",
      "Epoch [3720/10000], Loss: 0.6958\n",
      "Epoch [3730/10000], Loss: 0.6685\n",
      "Epoch [3740/10000], Loss: 0.3688\n",
      "Epoch [3750/10000], Loss: 0.6581\n",
      "Epoch [3760/10000], Loss: 0.4903\n",
      "Epoch [3770/10000], Loss: 0.5683\n",
      "Epoch [3780/10000], Loss: 0.5963\n",
      "Epoch [3790/10000], Loss: 0.7190\n",
      "Epoch [3800/10000], Loss: 0.5167\n",
      "Epoch [3810/10000], Loss: 0.4395\n",
      "Epoch [3820/10000], Loss: 0.6209\n",
      "Epoch [3830/10000], Loss: 0.5958\n",
      "Epoch [3840/10000], Loss: 0.5390\n",
      "Epoch [3850/10000], Loss: 0.4903\n",
      "Epoch [3860/10000], Loss: 0.2979\n",
      "Epoch [3870/10000], Loss: 0.3812\n",
      "Epoch [3880/10000], Loss: 0.5278\n",
      "Epoch [3890/10000], Loss: 0.6275\n",
      "Epoch [3900/10000], Loss: 0.5385\n",
      "Epoch [3910/10000], Loss: 0.5813\n",
      "Epoch [3920/10000], Loss: 0.4674\n",
      "Epoch [3930/10000], Loss: 0.4873\n",
      "Epoch [3940/10000], Loss: 0.4112\n",
      "Epoch [3950/10000], Loss: 0.5956\n",
      "Epoch [3960/10000], Loss: 0.4339\n",
      "Epoch [3970/10000], Loss: 0.5969\n",
      "Epoch [3980/10000], Loss: 0.4975\n",
      "Epoch [3990/10000], Loss: 0.3995\n",
      "Epoch [4000/10000], Loss: 0.4978\n",
      "Epoch [4010/10000], Loss: 0.7942\n",
      "Epoch [4020/10000], Loss: 0.5162\n",
      "Epoch [4030/10000], Loss: 0.5752\n",
      "Epoch [4040/10000], Loss: 0.5716\n",
      "Epoch [4050/10000], Loss: 0.3721\n",
      "Epoch [4060/10000], Loss: 0.4989\n",
      "Epoch [4070/10000], Loss: 0.5162\n",
      "Epoch [4080/10000], Loss: 0.5163\n",
      "Epoch [4090/10000], Loss: 0.4623\n",
      "Epoch [4100/10000], Loss: 0.6109\n",
      "Epoch [4110/10000], Loss: 0.5864\n",
      "Epoch [4120/10000], Loss: 0.5945\n",
      "Epoch [4130/10000], Loss: 0.5163\n",
      "Epoch [4140/10000], Loss: 0.5763\n",
      "Epoch [4150/10000], Loss: 0.6886\n",
      "Epoch [4160/10000], Loss: 0.6019\n",
      "Epoch [4170/10000], Loss: 0.5160\n",
      "Epoch [4180/10000], Loss: 0.6515\n",
      "Epoch [4190/10000], Loss: 0.5238\n",
      "Epoch [4200/10000], Loss: 0.5949\n",
      "Epoch [4210/10000], Loss: 0.5446\n",
      "Epoch [4220/10000], Loss: 0.3724\n",
      "Epoch [4230/10000], Loss: 0.5484\n",
      "Epoch [4240/10000], Loss: 0.5963\n",
      "Epoch [4250/10000], Loss: 0.5026\n",
      "Epoch [4260/10000], Loss: 0.4635\n",
      "Epoch [4270/10000], Loss: 0.5950\n",
      "Epoch [4280/10000], Loss: 0.4129\n",
      "Epoch [4290/10000], Loss: 0.5944\n",
      "Epoch [4300/10000], Loss: 0.6843\n",
      "Epoch [4310/10000], Loss: 0.6155\n",
      "Epoch [4320/10000], Loss: 0.3556\n",
      "Epoch [4330/10000], Loss: 0.5164\n",
      "Epoch [4340/10000], Loss: 0.7286\n",
      "Epoch [4350/10000], Loss: 0.4415\n",
      "Epoch [4360/10000], Loss: 0.6110\n",
      "Epoch [4370/10000], Loss: 0.5712\n",
      "Epoch [4380/10000], Loss: 0.5162\n",
      "Epoch [4390/10000], Loss: 0.5953\n",
      "Epoch [4400/10000], Loss: 0.5955\n",
      "Epoch [4410/10000], Loss: 0.4181\n",
      "Epoch [4420/10000], Loss: 0.6237\n",
      "Epoch [4430/10000], Loss: 0.3995\n",
      "Epoch [4440/10000], Loss: 0.5726\n",
      "Epoch [4450/10000], Loss: 0.7411\n",
      "Epoch [4460/10000], Loss: 0.6748\n",
      "Epoch [4470/10000], Loss: 0.7224\n",
      "Epoch [4480/10000], Loss: 0.4595\n",
      "Epoch [4490/10000], Loss: 0.6500\n",
      "Epoch [4500/10000], Loss: 0.5953\n",
      "Epoch [4510/10000], Loss: 0.7983\n",
      "Epoch [4520/10000], Loss: 0.6037\n",
      "Epoch [4530/10000], Loss: 0.5990\n",
      "Epoch [4540/10000], Loss: 0.4152\n",
      "Epoch [4550/10000], Loss: 0.5478\n",
      "Epoch [4560/10000], Loss: 0.4222\n",
      "Epoch [4570/10000], Loss: 0.5162\n",
      "Epoch [4580/10000], Loss: 0.4191\n",
      "Epoch [4590/10000], Loss: 0.5395\n",
      "Epoch [4600/10000], Loss: 0.6263\n",
      "Epoch [4610/10000], Loss: 0.5022\n",
      "Epoch [4620/10000], Loss: 0.6804\n",
      "Epoch [4630/10000], Loss: 0.6729\n",
      "Epoch [4640/10000], Loss: 0.8205\n",
      "Epoch [4650/10000], Loss: 0.5381\n",
      "Epoch [4660/10000], Loss: 0.5491\n",
      "Epoch [4670/10000], Loss: 0.5091\n",
      "Epoch [4680/10000], Loss: 0.4420\n",
      "Epoch [4690/10000], Loss: 0.4914\n",
      "Epoch [4700/10000], Loss: 0.6083\n",
      "Epoch [4710/10000], Loss: 0.8153\n",
      "Epoch [4720/10000], Loss: 0.5430\n",
      "Epoch [4730/10000], Loss: 0.5379\n",
      "Epoch [4740/10000], Loss: 0.4127\n",
      "Epoch [4750/10000], Loss: 0.5166\n",
      "Epoch [4760/10000], Loss: 0.4408\n",
      "Epoch [4770/10000], Loss: 0.5002\n",
      "Epoch [4780/10000], Loss: 0.3098\n",
      "Epoch [4790/10000], Loss: 0.6120\n",
      "Epoch [4800/10000], Loss: 0.6545\n",
      "Epoch [4810/10000], Loss: 0.5753\n",
      "Epoch [4820/10000], Loss: 0.5278\n",
      "Epoch [4830/10000], Loss: 0.6444\n",
      "Epoch [4840/10000], Loss: 0.5711\n",
      "Epoch [4850/10000], Loss: 0.6517\n",
      "Epoch [4860/10000], Loss: 0.4754\n",
      "Epoch [4870/10000], Loss: 0.8215\n",
      "Epoch [4880/10000], Loss: 0.6490\n",
      "Epoch [4890/10000], Loss: 0.5163\n",
      "Epoch [4900/10000], Loss: 0.7852\n",
      "Epoch [4910/10000], Loss: 0.5266\n",
      "Epoch [4920/10000], Loss: 0.6842\n",
      "Epoch [4930/10000], Loss: 0.5162\n",
      "Epoch [4940/10000], Loss: 0.5960\n",
      "Epoch [4950/10000], Loss: 0.5432\n",
      "Epoch [4960/10000], Loss: 0.4650\n",
      "Epoch [4970/10000], Loss: 0.4150\n",
      "Epoch [4980/10000], Loss: 0.2983\n",
      "Epoch [4990/10000], Loss: 0.4422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/10000], Loss: 0.5162\n",
      "Epoch [5010/10000], Loss: 0.5482\n",
      "Epoch [5020/10000], Loss: 0.7025\n",
      "Epoch [5030/10000], Loss: 0.6197\n",
      "Epoch [5040/10000], Loss: 0.5932\n",
      "Epoch [5050/10000], Loss: 0.3772\n",
      "Epoch [5060/10000], Loss: 0.4039\n",
      "Epoch [5070/10000], Loss: 0.4431\n",
      "Epoch [5080/10000], Loss: 0.5223\n",
      "Epoch [5090/10000], Loss: 0.4270\n",
      "Epoch [5100/10000], Loss: 0.6541\n",
      "Epoch [5110/10000], Loss: 0.4274\n",
      "Epoch [5120/10000], Loss: 0.6129\n",
      "Epoch [5130/10000], Loss: 0.5162\n",
      "Epoch [5140/10000], Loss: 0.5279\n",
      "Epoch [5150/10000], Loss: 0.5977\n",
      "Epoch [5160/10000], Loss: 0.6137\n",
      "Epoch [5170/10000], Loss: 0.6925\n",
      "Epoch [5180/10000], Loss: 0.6050\n",
      "Epoch [5190/10000], Loss: 0.7901\n",
      "Epoch [5200/10000], Loss: 0.6218\n",
      "Epoch [5210/10000], Loss: 0.5684\n",
      "Epoch [5220/10000], Loss: 0.4429\n",
      "Epoch [5230/10000], Loss: 0.5166\n",
      "Epoch [5240/10000], Loss: 0.4894\n",
      "Epoch [5250/10000], Loss: 0.4566\n",
      "Epoch [5260/10000], Loss: 0.5162\n",
      "Epoch [5270/10000], Loss: 0.5463\n",
      "Epoch [5280/10000], Loss: 0.6489\n",
      "Epoch [5290/10000], Loss: 0.5529\n",
      "Epoch [5300/10000], Loss: 0.4880\n",
      "Epoch [5310/10000], Loss: 0.4582\n",
      "Epoch [5320/10000], Loss: 0.5954\n",
      "Epoch [5330/10000], Loss: 0.4893\n",
      "Epoch [5340/10000], Loss: 0.3708\n",
      "Epoch [5350/10000], Loss: 0.4630\n",
      "Epoch [5360/10000], Loss: 0.4320\n",
      "Epoch [5370/10000], Loss: 0.6224\n",
      "Epoch [5380/10000], Loss: 0.4393\n",
      "Epoch [5390/10000], Loss: 0.7141\n",
      "Epoch [5400/10000], Loss: 0.4990\n",
      "Epoch [5410/10000], Loss: 0.4925\n",
      "Epoch [5420/10000], Loss: 0.3070\n",
      "Epoch [5430/10000], Loss: 0.5943\n",
      "Epoch [5440/10000], Loss: 0.3986\n",
      "Epoch [5450/10000], Loss: 0.3426\n",
      "Epoch [5460/10000], Loss: 0.8098\n",
      "Epoch [5470/10000], Loss: 0.4412\n",
      "Epoch [5480/10000], Loss: 0.5105\n",
      "Epoch [5490/10000], Loss: 0.2873\n",
      "Epoch [5500/10000], Loss: 0.4108\n",
      "Epoch [5510/10000], Loss: 0.5923\n",
      "Epoch [5520/10000], Loss: 0.5469\n",
      "Epoch [5530/10000], Loss: 0.5794\n",
      "Epoch [5540/10000], Loss: 0.3689\n",
      "Epoch [5550/10000], Loss: 0.6275\n",
      "Epoch [5560/10000], Loss: 0.5002\n",
      "Epoch [5570/10000], Loss: 0.6546\n",
      "Epoch [5580/10000], Loss: 0.6224\n",
      "Epoch [5590/10000], Loss: 0.6572\n",
      "Epoch [5600/10000], Loss: 0.5844\n",
      "Epoch [5610/10000], Loss: 0.4312\n",
      "Epoch [5620/10000], Loss: 0.4160\n",
      "Epoch [5630/10000], Loss: 0.5383\n",
      "Epoch [5640/10000], Loss: 0.4994\n",
      "Epoch [5650/10000], Loss: 0.4938\n",
      "Epoch [5660/10000], Loss: 0.5474\n",
      "Epoch [5670/10000], Loss: 0.5177\n",
      "Epoch [5680/10000], Loss: 0.6838\n",
      "Epoch [5690/10000], Loss: 0.7105\n",
      "Epoch [5700/10000], Loss: 0.5164\n",
      "Epoch [5710/10000], Loss: 0.6679\n",
      "Epoch [5720/10000], Loss: 0.5945\n",
      "Epoch [5730/10000], Loss: 0.6058\n",
      "Epoch [5740/10000], Loss: 0.3433\n",
      "Epoch [5750/10000], Loss: 0.6583\n",
      "Epoch [5760/10000], Loss: 0.6582\n",
      "Epoch [5770/10000], Loss: 0.6059\n",
      "Epoch [5780/10000], Loss: 0.6047\n",
      "Epoch [5790/10000], Loss: 0.5965\n",
      "Epoch [5800/10000], Loss: 0.4291\n",
      "Epoch [5810/10000], Loss: 0.4395\n",
      "Epoch [5820/10000], Loss: 0.7850\n",
      "Epoch [5830/10000], Loss: 0.3152\n",
      "Epoch [5840/10000], Loss: 0.6359\n",
      "Epoch [5850/10000], Loss: 0.4332\n",
      "Epoch [5860/10000], Loss: 0.4994\n",
      "Epoch [5870/10000], Loss: 0.5943\n",
      "Epoch [5880/10000], Loss: 0.4884\n",
      "Epoch [5890/10000], Loss: 0.5295\n",
      "Epoch [5900/10000], Loss: 0.4136\n",
      "Epoch [5910/10000], Loss: 0.4993\n",
      "Epoch [5920/10000], Loss: 0.4605\n",
      "Epoch [5930/10000], Loss: 0.5952\n",
      "Epoch [5940/10000], Loss: 0.4411\n",
      "Epoch [5950/10000], Loss: 0.5494\n",
      "Epoch [5960/10000], Loss: 0.5101\n",
      "Epoch [5970/10000], Loss: 0.5388\n",
      "Epoch [5980/10000], Loss: 0.5983\n",
      "Epoch [5990/10000], Loss: 0.6019\n",
      "Epoch [6000/10000], Loss: 0.4992\n",
      "Epoch [6010/10000], Loss: 0.6628\n",
      "Epoch [6020/10000], Loss: 0.6020\n",
      "Epoch [6030/10000], Loss: 0.5275\n",
      "Epoch [6040/10000], Loss: 0.3165\n",
      "Epoch [6050/10000], Loss: 0.5064\n",
      "Epoch [6060/10000], Loss: 0.4393\n",
      "Epoch [6070/10000], Loss: 0.3418\n",
      "Epoch [6080/10000], Loss: 0.4227\n",
      "Epoch [6090/10000], Loss: 0.3825\n",
      "Epoch [6100/10000], Loss: 0.6571\n",
      "Epoch [6110/10000], Loss: 0.6177\n",
      "Epoch [6120/10000], Loss: 0.5162\n",
      "Epoch [6130/10000], Loss: 0.5388\n",
      "Epoch [6140/10000], Loss: 0.6250\n",
      "Epoch [6150/10000], Loss: 0.5719\n",
      "Epoch [6160/10000], Loss: 0.5819\n",
      "Epoch [6170/10000], Loss: 0.5910\n",
      "Epoch [6180/10000], Loss: 0.6258\n",
      "Epoch [6190/10000], Loss: 0.6113\n",
      "Epoch [6200/10000], Loss: 0.3901\n",
      "Epoch [6210/10000], Loss: 0.5769\n",
      "Epoch [6220/10000], Loss: 0.6172\n",
      "Epoch [6230/10000], Loss: 0.5959\n",
      "Epoch [6240/10000], Loss: 0.6898\n",
      "Epoch [6250/10000], Loss: 0.5000\n",
      "Epoch [6260/10000], Loss: 0.5164\n",
      "Epoch [6270/10000], Loss: 0.5929\n",
      "Epoch [6280/10000], Loss: 0.4164\n",
      "Epoch [6290/10000], Loss: 0.4141\n",
      "Epoch [6300/10000], Loss: 0.5026\n",
      "Epoch [6310/10000], Loss: 0.4709\n",
      "Epoch [6320/10000], Loss: 0.4891\n",
      "Epoch [6330/10000], Loss: 0.4320\n",
      "Epoch [6340/10000], Loss: 0.4582\n",
      "Epoch [6350/10000], Loss: 0.5969\n",
      "Epoch [6360/10000], Loss: 0.5731\n",
      "Epoch [6370/10000], Loss: 0.4754\n",
      "Epoch [6380/10000], Loss: 0.3407\n",
      "Epoch [6390/10000], Loss: 0.7951\n",
      "Epoch [6400/10000], Loss: 0.4146\n",
      "Epoch [6410/10000], Loss: 0.5007\n",
      "Epoch [6420/10000], Loss: 0.5719\n",
      "Epoch [6430/10000], Loss: 0.6296\n",
      "Epoch [6440/10000], Loss: 0.6763\n",
      "Epoch [6450/10000], Loss: 0.5379\n",
      "Epoch [6460/10000], Loss: 0.6196\n",
      "Epoch [6470/10000], Loss: 0.3492\n",
      "Epoch [6480/10000], Loss: 0.6528\n",
      "Epoch [6490/10000], Loss: 0.5421\n",
      "Epoch [6500/10000], Loss: 0.5971\n",
      "Epoch [6510/10000], Loss: 0.5985\n",
      "Epoch [6520/10000], Loss: 0.5939\n",
      "Epoch [6530/10000], Loss: 0.5426\n",
      "Epoch [6540/10000], Loss: 0.4031\n",
      "Epoch [6550/10000], Loss: 0.5937\n",
      "Epoch [6560/10000], Loss: 0.6488\n",
      "Epoch [6570/10000], Loss: 0.4958\n",
      "Epoch [6580/10000], Loss: 0.4024\n",
      "Epoch [6590/10000], Loss: 0.4003\n",
      "Epoch [6600/10000], Loss: 0.4906\n",
      "Epoch [6610/10000], Loss: 0.4767\n",
      "Epoch [6620/10000], Loss: 0.4372\n",
      "Epoch [6630/10000], Loss: 0.4322\n",
      "Epoch [6640/10000], Loss: 0.5162\n",
      "Epoch [6650/10000], Loss: 0.3762\n",
      "Epoch [6660/10000], Loss: 0.5975\n",
      "Epoch [6670/10000], Loss: 0.4415\n",
      "Epoch [6680/10000], Loss: 0.6881\n",
      "Epoch [6690/10000], Loss: 0.4802\n",
      "Epoch [6700/10000], Loss: 0.5165\n",
      "Epoch [6710/10000], Loss: 0.6279\n",
      "Epoch [6720/10000], Loss: 0.4887\n",
      "Epoch [6730/10000], Loss: 0.6629\n",
      "Epoch [6740/10000], Loss: 0.5163\n",
      "Epoch [6750/10000], Loss: 0.3679\n",
      "Epoch [6760/10000], Loss: 0.5834\n",
      "Epoch [6770/10000], Loss: 0.3834\n",
      "Epoch [6780/10000], Loss: 0.3558\n",
      "Epoch [6790/10000], Loss: 0.4962\n",
      "Epoch [6800/10000], Loss: 0.3788\n",
      "Epoch [6810/10000], Loss: 0.3967\n",
      "Epoch [6820/10000], Loss: 0.6979\n",
      "Epoch [6830/10000], Loss: 0.5162\n",
      "Epoch [6840/10000], Loss: 0.6197\n",
      "Epoch [6850/10000], Loss: 0.5002\n",
      "Epoch [6860/10000], Loss: 0.4149\n",
      "Epoch [6870/10000], Loss: 0.6094\n",
      "Epoch [6880/10000], Loss: 0.6118\n",
      "Epoch [6890/10000], Loss: 0.6203\n",
      "Epoch [6900/10000], Loss: 0.6651\n",
      "Epoch [6910/10000], Loss: 0.4405\n",
      "Epoch [6920/10000], Loss: 0.6026\n",
      "Epoch [6930/10000], Loss: 0.4770\n",
      "Epoch [6940/10000], Loss: 0.3180\n",
      "Epoch [6950/10000], Loss: 0.6581\n",
      "Epoch [6960/10000], Loss: 0.5169\n",
      "Epoch [6970/10000], Loss: 0.6672\n",
      "Epoch [6980/10000], Loss: 0.5958\n",
      "Epoch [6990/10000], Loss: 0.6592\n",
      "Epoch [7000/10000], Loss: 0.5745\n",
      "Epoch [7010/10000], Loss: 0.4979\n",
      "Epoch [7020/10000], Loss: 0.4413\n",
      "Epoch [7030/10000], Loss: 0.6260\n",
      "Epoch [7040/10000], Loss: 0.6793\n",
      "Epoch [7050/10000], Loss: 0.4106\n",
      "Epoch [7060/10000], Loss: 0.6244\n",
      "Epoch [7070/10000], Loss: 0.5841\n",
      "Epoch [7080/10000], Loss: 0.6814\n",
      "Epoch [7090/10000], Loss: 0.4309\n",
      "Epoch [7100/10000], Loss: 0.7060\n",
      "Epoch [7110/10000], Loss: 0.4258\n",
      "Epoch [7120/10000], Loss: 0.3756\n",
      "Epoch [7130/10000], Loss: 0.3977\n",
      "Epoch [7140/10000], Loss: 0.4424\n",
      "Epoch [7150/10000], Loss: 0.3850\n",
      "Epoch [7160/10000], Loss: 0.4404\n",
      "Epoch [7170/10000], Loss: 0.4970\n",
      "Epoch [7180/10000], Loss: 0.5720\n",
      "Epoch [7190/10000], Loss: 0.4149\n",
      "Epoch [7200/10000], Loss: 0.5101\n",
      "Epoch [7210/10000], Loss: 0.5383\n",
      "Epoch [7220/10000], Loss: 0.5969\n",
      "Epoch [7230/10000], Loss: 0.8081\n",
      "Epoch [7240/10000], Loss: 0.5169\n",
      "Epoch [7250/10000], Loss: 0.5132\n",
      "Epoch [7260/10000], Loss: 0.6634\n",
      "Epoch [7270/10000], Loss: 0.4046\n",
      "Epoch [7280/10000], Loss: 0.4623\n",
      "Epoch [7290/10000], Loss: 0.5459\n",
      "Epoch [7300/10000], Loss: 0.5102\n",
      "Epoch [7310/10000], Loss: 0.5762\n",
      "Epoch [7320/10000], Loss: 0.6141\n",
      "Epoch [7330/10000], Loss: 0.5891\n",
      "Epoch [7340/10000], Loss: 0.6023\n",
      "Epoch [7350/10000], Loss: 0.6846\n",
      "Epoch [7360/10000], Loss: 0.4896\n",
      "Epoch [7370/10000], Loss: 0.6017\n",
      "Epoch [7380/10000], Loss: 0.6784\n",
      "Epoch [7390/10000], Loss: 0.5720\n",
      "Epoch [7400/10000], Loss: 0.6373\n",
      "Epoch [7410/10000], Loss: 0.6060\n",
      "Epoch [7420/10000], Loss: 0.7031\n",
      "Epoch [7430/10000], Loss: 0.6754\n",
      "Epoch [7440/10000], Loss: 0.5456\n",
      "Epoch [7450/10000], Loss: 0.6585\n",
      "Epoch [7460/10000], Loss: 0.6828\n",
      "Epoch [7470/10000], Loss: 0.5162\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7480/10000], Loss: 0.6650\n",
      "Epoch [7490/10000], Loss: 0.6255\n",
      "Epoch [7500/10000], Loss: 0.5353\n",
      "Epoch [7510/10000], Loss: 0.6677\n",
      "Epoch [7520/10000], Loss: 0.5162\n",
      "Epoch [7530/10000], Loss: 0.5479\n",
      "Epoch [7540/10000], Loss: 0.6544\n",
      "Epoch [7550/10000], Loss: 0.4416\n",
      "Epoch [7560/10000], Loss: 0.4408\n",
      "Epoch [7570/10000], Loss: 0.4888\n",
      "Epoch [7580/10000], Loss: 0.3250\n",
      "Epoch [7590/10000], Loss: 0.4184\n",
      "Epoch [7600/10000], Loss: 0.5162\n",
      "Epoch [7610/10000], Loss: 0.5914\n",
      "Epoch [7620/10000], Loss: 0.5946\n",
      "Epoch [7630/10000], Loss: 0.4981\n",
      "Epoch [7640/10000], Loss: 0.5640\n",
      "Epoch [7650/10000], Loss: 0.4892\n",
      "Epoch [7660/10000], Loss: 0.6137\n",
      "Epoch [7670/10000], Loss: 0.5007\n",
      "Epoch [7680/10000], Loss: 0.3696\n",
      "Epoch [7690/10000], Loss: 0.8097\n",
      "Epoch [7700/10000], Loss: 0.5163\n",
      "Epoch [7710/10000], Loss: 0.6155\n",
      "Epoch [7720/10000], Loss: 0.5274\n",
      "Epoch [7730/10000], Loss: 0.4723\n",
      "Epoch [7740/10000], Loss: 0.6147\n",
      "Epoch [7750/10000], Loss: 0.7324\n",
      "Epoch [7760/10000], Loss: 0.6713\n",
      "Epoch [7770/10000], Loss: 0.5884\n",
      "Epoch [7780/10000], Loss: 0.4379\n",
      "Epoch [7790/10000], Loss: 0.4144\n",
      "Epoch [7800/10000], Loss: 0.5883\n",
      "Epoch [7810/10000], Loss: 0.5273\n",
      "Epoch [7820/10000], Loss: 0.5942\n",
      "Epoch [7830/10000], Loss: 0.6051\n",
      "Epoch [7840/10000], Loss: 0.5473\n",
      "Epoch [7850/10000], Loss: 0.5162\n",
      "Epoch [7860/10000], Loss: 0.6642\n",
      "Epoch [7870/10000], Loss: 0.5220\n",
      "Epoch [7880/10000], Loss: 0.3769\n",
      "Epoch [7890/10000], Loss: 0.7834\n",
      "Epoch [7900/10000], Loss: 0.6052\n",
      "Epoch [7910/10000], Loss: 0.3740\n",
      "Epoch [7920/10000], Loss: 0.4050\n",
      "Epoch [7930/10000], Loss: 0.6079\n",
      "Epoch [7940/10000], Loss: 0.6036\n",
      "Epoch [7950/10000], Loss: 0.5412\n",
      "Epoch [7960/10000], Loss: 0.4826\n",
      "Epoch [7970/10000], Loss: 0.5876\n",
      "Epoch [7980/10000], Loss: 0.6182\n",
      "Epoch [7990/10000], Loss: 0.6524\n",
      "Epoch [8000/10000], Loss: 0.6970\n",
      "Epoch [8010/10000], Loss: 0.5470\n",
      "Epoch [8020/10000], Loss: 0.5162\n",
      "Epoch [8030/10000], Loss: 0.5162\n",
      "Epoch [8040/10000], Loss: 0.6225\n",
      "Epoch [8050/10000], Loss: 0.5238\n",
      "Epoch [8060/10000], Loss: 0.4856\n",
      "Epoch [8070/10000], Loss: 0.5065\n",
      "Epoch [8080/10000], Loss: 0.6313\n",
      "Epoch [8090/10000], Loss: 0.4587\n",
      "Epoch [8100/10000], Loss: 0.4154\n",
      "Epoch [8110/10000], Loss: 0.8073\n",
      "Epoch [8120/10000], Loss: 0.6662\n",
      "Epoch [8130/10000], Loss: 0.5888\n",
      "Epoch [8140/10000], Loss: 0.6581\n",
      "Epoch [8150/10000], Loss: 0.5168\n",
      "Epoch [8160/10000], Loss: 0.4028\n",
      "Epoch [8170/10000], Loss: 0.8066\n",
      "Epoch [8180/10000], Loss: 0.5060\n",
      "Epoch [8190/10000], Loss: 0.5457\n",
      "Epoch [8200/10000], Loss: 0.5949\n",
      "Epoch [8210/10000], Loss: 0.6415\n",
      "Epoch [8220/10000], Loss: 0.4739\n",
      "Epoch [8230/10000], Loss: 0.4121\n",
      "Epoch [8240/10000], Loss: 0.8110\n",
      "Epoch [8250/10000], Loss: 0.5974\n",
      "Epoch [8260/10000], Loss: 0.5277\n",
      "Epoch [8270/10000], Loss: 0.5702\n",
      "Epoch [8280/10000], Loss: 0.5105\n",
      "Epoch [8290/10000], Loss: 0.6675\n",
      "Epoch [8300/10000], Loss: 0.4645\n",
      "Epoch [8310/10000], Loss: 0.3199\n",
      "Epoch [8320/10000], Loss: 0.5366\n",
      "Epoch [8330/10000], Loss: 0.5942\n",
      "Epoch [8340/10000], Loss: 0.6734\n",
      "Epoch [8350/10000], Loss: 0.4523\n",
      "Epoch [8360/10000], Loss: 0.3794\n",
      "Epoch [8370/10000], Loss: 0.7051\n",
      "Epoch [8380/10000], Loss: 0.4617\n",
      "Epoch [8390/10000], Loss: 0.6618\n",
      "Epoch [8400/10000], Loss: 0.6086\n",
      "Epoch [8410/10000], Loss: 0.4022\n",
      "Epoch [8420/10000], Loss: 0.7120\n",
      "Epoch [8430/10000], Loss: 0.4128\n",
      "Epoch [8440/10000], Loss: 0.4897\n",
      "Epoch [8450/10000], Loss: 0.6232\n",
      "Epoch [8460/10000], Loss: 0.4818\n",
      "Epoch [8470/10000], Loss: 0.4423\n",
      "Epoch [8480/10000], Loss: 0.5165\n",
      "Epoch [8490/10000], Loss: 0.5387\n",
      "Epoch [8500/10000], Loss: 0.6594\n",
      "Epoch [8510/10000], Loss: 0.6495\n",
      "Epoch [8520/10000], Loss: 0.5871\n",
      "Epoch [8530/10000], Loss: 0.4493\n",
      "Epoch [8540/10000], Loss: 0.6786\n",
      "Epoch [8550/10000], Loss: 0.4983\n",
      "Epoch [8560/10000], Loss: 0.5712\n",
      "Epoch [8570/10000], Loss: 0.4587\n",
      "Epoch [8580/10000], Loss: 0.6556\n",
      "Epoch [8590/10000], Loss: 0.5385\n",
      "Epoch [8600/10000], Loss: 0.5877\n",
      "Epoch [8610/10000], Loss: 0.8183\n",
      "Epoch [8620/10000], Loss: 0.3769\n",
      "Epoch [8630/10000], Loss: 0.6207\n",
      "Epoch [8640/10000], Loss: 0.3883\n",
      "Epoch [8650/10000], Loss: 0.5459\n",
      "Epoch [8660/10000], Loss: 0.5372\n",
      "Epoch [8670/10000], Loss: 0.6513\n",
      "Epoch [8680/10000], Loss: 0.5166\n",
      "Epoch [8690/10000], Loss: 0.6692\n",
      "Epoch [8700/10000], Loss: 0.6727\n",
      "Epoch [8710/10000], Loss: 0.6796\n",
      "Epoch [8720/10000], Loss: 0.5689\n",
      "Epoch [8730/10000], Loss: 0.5099\n",
      "Epoch [8740/10000], Loss: 0.5844\n",
      "Epoch [8750/10000], Loss: 0.6189\n",
      "Epoch [8760/10000], Loss: 0.6108\n",
      "Epoch [8770/10000], Loss: 0.4004\n",
      "Epoch [8780/10000], Loss: 0.4233\n",
      "Epoch [8790/10000], Loss: 0.6641\n",
      "Epoch [8800/10000], Loss: 0.6114\n",
      "Epoch [8810/10000], Loss: 0.3841\n",
      "Epoch [8820/10000], Loss: 0.6318\n",
      "Epoch [8830/10000], Loss: 0.3725\n",
      "Epoch [8840/10000], Loss: 0.5465\n",
      "Epoch [8850/10000], Loss: 0.5482\n",
      "Epoch [8860/10000], Loss: 0.4149\n",
      "Epoch [8870/10000], Loss: 0.6164\n",
      "Epoch [8880/10000], Loss: 0.4004\n",
      "Epoch [8890/10000], Loss: 0.6307\n",
      "Epoch [8900/10000], Loss: 0.5392\n",
      "Epoch [8910/10000], Loss: 0.5621\n",
      "Epoch [8920/10000], Loss: 0.3331\n",
      "Epoch [8930/10000], Loss: 0.4144\n",
      "Epoch [8940/10000], Loss: 0.4623\n",
      "Epoch [8950/10000], Loss: 0.4635\n",
      "Epoch [8960/10000], Loss: 0.8073\n",
      "Epoch [8970/10000], Loss: 0.6155\n",
      "Epoch [8980/10000], Loss: 0.6070\n",
      "Epoch [8990/10000], Loss: 0.6315\n",
      "Epoch [9000/10000], Loss: 0.6751\n",
      "Epoch [9010/10000], Loss: 0.6219\n",
      "Epoch [9020/10000], Loss: 0.4048\n",
      "Epoch [9030/10000], Loss: 0.2882\n",
      "Epoch [9040/10000], Loss: 0.4897\n",
      "Epoch [9050/10000], Loss: 0.5162\n",
      "Epoch [9060/10000], Loss: 0.6245\n",
      "Epoch [9070/10000], Loss: 0.3796\n",
      "Epoch [9080/10000], Loss: 0.7051\n",
      "Epoch [9090/10000], Loss: 0.3879\n",
      "Epoch [9100/10000], Loss: 0.5949\n",
      "Epoch [9110/10000], Loss: 0.5162\n",
      "Epoch [9120/10000], Loss: 0.5165\n",
      "Epoch [9130/10000], Loss: 0.6229\n",
      "Epoch [9140/10000], Loss: 0.6958\n",
      "Epoch [9150/10000], Loss: 0.4137\n",
      "Epoch [9160/10000], Loss: 0.6498\n",
      "Epoch [9170/10000], Loss: 0.5964\n",
      "Epoch [9180/10000], Loss: 0.5928\n",
      "Epoch [9190/10000], Loss: 0.4382\n",
      "Epoch [9200/10000], Loss: 0.4129\n",
      "Epoch [9210/10000], Loss: 0.6729\n",
      "Epoch [9220/10000], Loss: 0.4977\n",
      "Epoch [9230/10000], Loss: 0.5951\n",
      "Epoch [9240/10000], Loss: 0.5195\n",
      "Epoch [9250/10000], Loss: 0.6813\n",
      "Epoch [9260/10000], Loss: 0.6502\n",
      "Epoch [9270/10000], Loss: 0.4407\n",
      "Epoch [9280/10000], Loss: 0.5096\n",
      "Epoch [9290/10000], Loss: 0.3729\n",
      "Epoch [9300/10000], Loss: 0.5729\n",
      "Epoch [9310/10000], Loss: 0.5476\n",
      "Epoch [9320/10000], Loss: 0.6832\n",
      "Epoch [9330/10000], Loss: 0.8067\n",
      "Epoch [9340/10000], Loss: 0.6037\n",
      "Epoch [9350/10000], Loss: 0.4092\n",
      "Epoch [9360/10000], Loss: 0.5162\n",
      "Epoch [9370/10000], Loss: 0.6673\n",
      "Epoch [9380/10000], Loss: 0.5147\n",
      "Epoch [9390/10000], Loss: 0.5164\n",
      "Epoch [9400/10000], Loss: 0.7920\n",
      "Epoch [9410/10000], Loss: 0.6011\n",
      "Epoch [9420/10000], Loss: 0.8122\n",
      "Epoch [9430/10000], Loss: 0.6194\n",
      "Epoch [9440/10000], Loss: 0.5163\n",
      "Epoch [9450/10000], Loss: 0.4062\n",
      "Epoch [9460/10000], Loss: 0.4243\n",
      "Epoch [9470/10000], Loss: 0.6060\n",
      "Epoch [9480/10000], Loss: 0.6090\n",
      "Epoch [9490/10000], Loss: 0.3442\n",
      "Epoch [9500/10000], Loss: 0.6096\n",
      "Epoch [9510/10000], Loss: 0.6068\n",
      "Epoch [9520/10000], Loss: 0.5906\n",
      "Epoch [9530/10000], Loss: 0.6133\n",
      "Epoch [9540/10000], Loss: 0.3739\n",
      "Epoch [9550/10000], Loss: 0.5929\n",
      "Epoch [9560/10000], Loss: 0.5938\n",
      "Epoch [9570/10000], Loss: 0.7739\n",
      "Epoch [9580/10000], Loss: 0.5164\n",
      "Epoch [9590/10000], Loss: 0.7565\n",
      "Epoch [9600/10000], Loss: 0.3670\n",
      "Epoch [9610/10000], Loss: 0.8003\n",
      "Epoch [9620/10000], Loss: 0.4918\n",
      "Epoch [9630/10000], Loss: 0.3827\n",
      "Epoch [9640/10000], Loss: 0.6796\n",
      "Epoch [9650/10000], Loss: 0.5164\n",
      "Epoch [9660/10000], Loss: 0.4715\n",
      "Epoch [9670/10000], Loss: 0.5309\n",
      "Epoch [9680/10000], Loss: 0.3445\n",
      "Epoch [9690/10000], Loss: 0.4412\n",
      "Epoch [9700/10000], Loss: 0.6015\n",
      "Epoch [9710/10000], Loss: 0.5475\n",
      "Epoch [9720/10000], Loss: 0.5161\n",
      "Epoch [9730/10000], Loss: 0.2561\n",
      "Epoch [9740/10000], Loss: 0.5019\n",
      "Epoch [9750/10000], Loss: 0.5375\n",
      "Epoch [9760/10000], Loss: 0.6708\n",
      "Epoch [9770/10000], Loss: 0.5331\n",
      "Epoch [9780/10000], Loss: 0.5356\n",
      "Epoch [9790/10000], Loss: 0.5215\n",
      "Epoch [9800/10000], Loss: 0.6193\n",
      "Epoch [9810/10000], Loss: 0.6767\n",
      "Epoch [9820/10000], Loss: 0.5934\n",
      "Epoch [9830/10000], Loss: 0.4675\n",
      "Epoch [9840/10000], Loss: 0.3725\n",
      "Epoch [9850/10000], Loss: 0.4830\n",
      "Epoch [9860/10000], Loss: 0.7272\n",
      "Epoch [9870/10000], Loss: 0.5415\n",
      "Epoch [9880/10000], Loss: 0.5284\n",
      "Epoch [9890/10000], Loss: 0.5277\n",
      "Epoch [9900/10000], Loss: 0.5164\n",
      "Epoch [9910/10000], Loss: 0.4590\n",
      "Epoch [9920/10000], Loss: 0.6535\n",
      "Epoch [9930/10000], Loss: 0.5474\n",
      "Epoch [9940/10000], Loss: 0.4173\n",
      "Epoch [9950/10000], Loss: 0.6510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9960/10000], Loss: 0.6278\n",
      "Epoch [9970/10000], Loss: 0.5162\n",
      "Epoch [9980/10000], Loss: 0.4026\n",
      "Epoch [9990/10000], Loss: 0.9462\n",
      "Epoch [10000/10000], Loss: 0.5932\n"
     ]
    }
   ],
   "source": [
    "fit(10000, model, loss_fn, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2215,  70.2559],\n",
       "        [ 82.1209, 100.7117],\n",
       "        [118.6412, 132.9734],\n",
       "        [ 21.0449,  37.0273],\n",
       "        [101.9004, 119.1337],\n",
       "        [ 57.2215,  70.2559],\n",
       "        [ 82.1209, 100.7117],\n",
       "        [118.6412, 132.9734],\n",
       "        [ 21.0449,  37.0273],\n",
       "        [101.9004, 119.1337],\n",
       "        [ 57.2215,  70.2559],\n",
       "        [ 82.1209, 100.7117],\n",
       "        [118.6412, 132.9734],\n",
       "        [ 21.0449,  37.0273],\n",
       "        [101.9004, 119.1337]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
